<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0042)https://ibug.doc.ic.ac.uk/resources/300-W/ -->
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013</title>
<meta name="description" content="">
<meta name="keywords" content="">
<script type="text/javascript" src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/jquery-1.3.2.min.js.다운로드"></script>
<link rel="stylesheet" type="text/css" href="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/content.css">
<link rel="stylesheet" type="text/css" href="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/960.css">
<link rel="stylesheet" type="text/css" href="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/ibug.css">

</head>
<body>

<div class="container_12">
    <div class="grid_12">
        <div id="banner">
            <div id="admin_link">
                
                <a href="https://ibug.doc.ic.ac.uk/accounts/login/">login</a>
                
            </div>
            <ul id="menu">
            
                <li><a href="https://ibug.doc.ic.ac.uk/home">home</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/news/">news</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/projects">projects</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/research">research</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/people">people</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/publications">publications</a></li>
            
                <li class="selected"><a href="https://ibug.doc.ic.ac.uk/resources">resources</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/vacancies">vacancies</a></li>
            
                <li><a href="https://ibug.doc.ic.ac.uk/courses">courses</a></li>
            
            </ul>
        </div>
    </div>
    <div class="grid_12" id="breadcrumb">
        <ul>
            
    <li><a href="https://ibug.doc.ic.ac.uk/">home</a></li>
    <li>» <a href="https://ibug.doc.ic.ac.uk/resources">resources</a></li>
    <li>» <a href="https://ibug.doc.ic.ac.uk/resources/300-W/">300 Faces In-the-Wild Challenge (300-W), ICCV 2013</a></li>

        </ul>
    </div>
    <div class="clear"> </div>
<!--    <div class="grid_12">-->
<!--        <h1>Resources</h1>-->
<!--    </div>-->
    
<div class="grid_4" id="sidebar">
    <ul>
    
        
        <li><p>Datasets</p>
        <ul>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">1st 3D Face Tracking in-the-wild Competition</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/bodypose-anno-correction/">Body Pose Annotations Correction (CVPR 2016)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/itwmm/">In-The-Wild 3D Morphable Models: Code and Data</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/SOP/">Sound of Pixels</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/av-database-normal-whispered-silent-speech/">Audiovisual Database of Normal-Whispered-Silent Speech  </a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/ibug-ears/">Deformable Models of Ears in-the-wild for Alignment and Recognition</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/300-VW/">300 Videos in the Wild (300-VW) Challenge &amp; Workshop (ICCV 2015)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/fabrics/">The Fabrics Dataset</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/Affect-W/">Affect "in-the-wild" Workshop</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">2nd Facial Landmark Localisation Competition - The Menpo BenchMark</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/lsfm/">Large Scale Facial Model (LSFM)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/agedb/">AgeDB</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/afew-va-database/">AFEW-VA Database for Valence and Arousal Estimation In-The-Wild</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/confer/">The CONFER Database</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/SI-HBAW/">Special Issue on Behavior Analysis "in-the-wild"</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge/">First Affect-in-the-Wild Challenge</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/first-faces-wild-workshop-challenge/">First Faces in-the-wild Workshop-Challenge</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/sewa-database/">The SEWA Database</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/FERA15/">Facial Expression Recognition and Analysis Challenge 2015</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/">300 Faces In-The-Wild Challenge (300-W), IMAVIS 2014</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/mimic-me/">Mimic Me</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/mahnob-hci-tagging-database/">MAHNOB-HCI-Tagging database</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/300-W/" class="selected">300 Faces In-the-Wild Challenge (300-W), ICCV 2013</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/mahnob-laughter-database2/">MAHNOB Laughter database</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/mahnob-mhi-mimicry-database/">MAHNOB MHI-Mimicry database</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/">Facial point annotations</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/mmi-facial-expression-database/">MMI Facial expression database</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/semaine-database2/">SEMAINE database</a></li>
            
        </ul>
        </li>
        
    
        
        <li><p>Code</p>
        <ul>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/menpo-project/">The Menpo Project</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/valencearousal-online-annotation-tool/">Valence/Arousal Online Annotation Tool</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/drmf-matlab-code-cvpr-2013/">Discriminative Response Map Fitting (DRMF 2013)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/gauss-newton-deformable-part-models-face-alignment/">Gauss-Newton Deformable Part Models for Face Alignment in-the-Wild (CVPR 2014)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/EndToEndLipreading/">End-to-End Lipreading</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/aoms-generic-face-alignment/">AOMs Generic Face Alignment (2012)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/fitting-aams-wild-iccv-2013/">Fitting AAMs in-the-Wild (ICCV 2013)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/dica/">Discriminant Incoherent Component Analysis (IEEE-TIP 2016)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/subspace-learning-image-gradient-orientations-igo-/">Subspace Learning from Image Gradient Orientations (2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/ele_TIP15/">DS-GPLVM (TIP 2015)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/robust-and-efficient-parametric-face-alignment/">Robust and Efficient Parametric Face/Object Alignment (2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/DOC-Toolbox/">The Dynamic Ordinal Classification (DOC) Toolbox</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/facial-point-detector-2010/">Facial point detector (2010/2013)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/chehra-tracker-cvpr-2014/">Chehra Face Tracker (CVPR 2014)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/empirical-analysis-cascade-deformable-models-multi/">Empirical Analysis Of Cascade Deformable Models For Multi-View Face Detection (IMAVIS 2015)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/salient-point-detector-20062008/">Salient Point Detector (2006/2008)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/real-time-face-tracking-cuda-mmsys-2014/">Real-time Face tracking with CUDA (MMSys 2014)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/fiducial-facial-point-detector-20052007/">Facial Point detector (2005/2007)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/facial-tracker-2011/">Facial tracker (2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/salient-point-detector-2010/">Salient Point Detector (2010)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/continuous-time-prediction-behavior-affect/">Continuous-time Prediction of Dimensional Behavior/Affect</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/action-unit-detector-2016/">Action Unit Detector (2016)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/laud-programme-20102011/">AU detector (LAUD 2010)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/temporal-based-action-unit-detection/">AU detector (TAUD 2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/gesture-detector-2010/">Gesture Detector (2010)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/nod-shake-detector-and-5-dimensional-emotion-predi/">Head Nod Shake Detector and 5 Dimensional Emotion Predictor (2010/2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/gesture-detector-2011/">Gesture Detector (2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/nod-shake-detector/">Head Nod Shake Detector (2010/2011)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/smile-detectors/">Smile Detectors</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/frog-facial-tracking-component-code/">FROG Facial Tracking Component</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/hci2-framework/">HCI^2 Framework</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/semaine-visual-components-20092010/">SEMAINE Visual Components (2009/2010)</a></li>
            
            <li><a href="https://ibug.doc.ic.ac.uk/resources/semaine-visual-components-20082009/">SEMAINE Visual Components (2008/2009)</a></li>
            
        </ul>
        </li>
        
    
    </ul>
</div>
<div class="grid_8">
    <div class="research">
        <h1>300 Faces In-the-Wild Challenge (300-W), ICCV 2013</h1>
        <div class="transbox">
            
                
            
        </div>
        <div class="transbox mce">
            <p align="center">&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Latest News!</strong></span></p>
<ul>
<li>The 300-W&nbsp;dataset has been released and can be downloaded from&nbsp;<span>[</span><a href="https://ibug.doc.ic.ac.uk/download/annotations/300w.zip.001">part1</a><span>][</span><a href="https://ibug.doc.ic.ac.uk/download/annotations/300w.zip.002">part2</a><span>][</span><a href="https://ibug.doc.ic.ac.uk/download/annotations/300w.zip.003">part3</a><span>][</span><a href="https://ibug.doc.ic.ac.uk/download/annotations/300w.zip.004">part4</a><span>].</span><a><br></a><span>Please note that the database is simply split into 4 smaller parts for easier download. In order to create the database you have to unzip part1 (i.e., 300w.zip.001) using a file archiver (e.g., 7zip).</span></li>
<li><span>The data are provided for research purposes only. <strong>Commercial use (i.e., use in training commercial algorithms) is not allowed.</strong></span></li>
<li>If you use the above dataset please cite the following papers:
<ul>
<li>
<p>C. Sagonas, E. Antonakos, G, Tzimiropoulos, S. Zafeiriou, M. Pantic.&nbsp;<a href="https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf"><strong style="margin: 0px; padding: 0px; font-size: 1em; line-height: 1em; list-style: none;">300 faces In-the-wild challenge: Database and results</strong></a><span style="font-size: 1em; line-height: 1.5em;">.&nbsp;Image and Vision Computing (IMAVIS),&nbsp;Special Issue on Facial Landmark Localisation "In-The-Wild". 2016.</span></p>
</li>
<li>
<p>C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, M. Pantic. <a href="https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_iccv_2013_300_w.pdf"><strong>300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge</strong></a>. Proceedings of IEEE Int’l Conf. on Computer Vision (ICCV-W), 300 Faces in-the-Wild Challenge (300-W). Sydney, Australia, December 2013.</p>
</li>
<li>
<p>C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, M. Pantic. <a href="https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_cvpr_2013_amfg_w.pdf"><strong>A semi-automatic methodology for facial landmark annotation</strong></a>. Proceedings of IEEE Int’l Conf. Computer Vision and Pattern Recognition (CVPR-W), 5th Workshop on Analysis and Modeling of Faces and Gestures (AMFG 2013). Oregon, USA, June 2013.</p>
</li>
</ul>
</li>
<li>Scripts in Matlab, Python and data to generate the results of both versions of the 300W Challenge (ICCV 2013, IMAVIS 2015) in the form of Cumulative Error Distribution (CED) curve can be downloaded from&nbsp;<a href="https://ibug.doc.ic.ac.uk/media/uploads/competitions/300w_results.zip">here</a>.&nbsp;</li>
<li>Second version of the 300 Faces In-the-Wild Challenge announced for a special issue in Elsevier Image and Vision Computing Journal!! Please see <a href="https://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/" target="_blank">here</a> for more details about 300-W 2014!</li>
<li><span>A call for papers for a new competition in a journal will be shortly announced. Please keep checking the site for the call. In this special issue, we will run the competition in a similar manner that we did with the competition 300-W.</span></li>
<li><span>Workshop was held on December 7th!&nbsp;<span>Please check the&nbsp;</span><strong>Results </strong><span>session for more information about participants, winners and results!</span></span>&nbsp;</li>
<li>The testing database will be released as soon as the IMAVIS papers of the second conduct of the competition<span>&nbsp;(</span><a href="https://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/" target="_blank">300-W IMAVIS</a><span>) get accepted.&nbsp;<span>In the meantime, if you wish to compare with the results of this conduct of the competition</span><span>, feel free to send us your binary code (</span><a href="mailto:300faces.challenge@gmail.com" target="_blank">300faces.challenge@gmail.com</a><span>) and we will send you back the final results.</span></span></li>
</ul>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>300-W</strong></span></p>
<p>The first Automatic Facial Landmark Detection in-the-Wild Challenge (300-W 2013) to be held in conjunction with International Conference on Computer Vision 2013, Sydney, Australia.</p>
<p><strong><br></strong></p>
<p><span style="text-decoration: underline;"><strong>Organisers</strong></span></p>
<p>Georgios Tzimiropoulos, University of Lincoln, UK<br>Stefanos Zafeiriou, Imperial College London, UK<br>Maja Pantic, Imperial College London, UK</p>
<p><strong><br></strong></p>
<p><span style="text-decoration: underline;"><strong>Scope</strong></span></p>
<p>Automatic facial landmark detection is a longstanding problem in computer vision, and 300-W Challenge is the first event of its kind organized exclusively to benchmark the efforts in the field. The particular focus is on facial landmark detection in real-world datasets of facial images captured in-the-wild. The results of the Challenge will be presented at the 300-W Faces in-the-Wild Workshop to be held in conjunction with ICCV 2013.</p>
<p>A special issue of Image and Vision Computing Journal will present the best performing methods and summarize the results of the Challenge.</p>
<p><strong><br></strong></p>
<p><span style="text-decoration: underline;"><strong>The 300-W Challenge</strong></span></p>
<p>Landmark annotations (following the Multi-PIE [1] 68 points mark-up, please see Fig. 1) for four popular data sets are available from <a class="internal" href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/">here</a>. All participants in the Challenge will be able to train their algorithms using these data. Performance evaluation will be carried out on 300-W test set, using the same Multi-PIE mark-up, and the same face-bounding box initialization.</p>
<p>&nbsp;</p>
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_1_68.jpg" alt="" width="300" height="242"></td>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_1_51.jpg" alt="" width="300" height="242"></td>
</tr>
</tbody>
</table>
<p style="text-align: center;">&nbsp;</p>
<p style="text-align: center;">&nbsp;Figure 1: The 68 and 51 points mark-up used for our annotations.</p>
<p align="center">&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Training</strong></span></p>
<p>The datasets LFPW [2], AFW [3], HELEN [4], and XM2VTS [5] have been re-annotated using the mark-up of Fig 1. We provide additional annotations for another 135 images in difficult poses and expressions (IBUG training set). Annotations have the same name as the corresponding images. For LFPW, AFW, HELEN, &nbsp;and IBUG &nbsp;datasets we also provide the images. The remaining image databases can be downloaded from the authors’ websites. All annotations can be downloaded from <a class="internal" href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/">here</a>.</p>
<p>Participants are strongly encouraged to train their algorithms using these training data. Should you use any of the provided annotations please cite [6] and the paper presenting the corresponding database.<strong>&nbsp;</strong></p>
<p><em>Please note that the re-annotated data for this challenge are saved in the matlab convention of 1 being </em><br><em>the first index, i.e. the coordinates of the top left pixel in an image are x=1, y=1.</em></p>
<p><em><br></em></p>
<p><span style="text-decoration: underline;"><strong>Testing</strong></span></p>
<p>Participants will have their algorithms tested on a newly collected data set with 2x300 (300 indoor and 300 outdoor) face images collected in the wild (300-W test set). Sample images are shown in Fig 2 and Fig 3. &nbsp;</p>
<p>&nbsp;</p>
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_2.jpg" alt="" width="300" height="220"></td>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_3.jpg" alt="" width="300" height="220"></td>
</tr>
<tr>
<td style="text-align: center;"><br>Figure 2: Outdoor.</td>
<td style="text-align: center;"><br>Figure 3: Indoor.</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>300-W test set is aimed to test the ability of current systems to handle unseen subjects, independently of variations in pose, expression, illumination, background, occlusion, and image quality.</p>
<p><strong><span style="text-decoration: underline;">Participants should send binaries</span></strong> with their trained algorithms to the organisers, who will run each algorithm on the 300-W test set using the same bounding box initialization. This bounding box is provided by our in-house face detector. The face region that our detector was trained on is defined by the bounding box as computed by the landmark annotations (please see Fig. 4).</p>
<p align="center">&nbsp;</p>
<p style="text-align: center;">&nbsp;&nbsp;<img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_4_n_2.png" alt="" width="300" height="267"></p>
<p style="text-align: center;">&nbsp;</p>
<p style="text-align: center;">&nbsp;Figure 4: Face region (bounding box) that our face detector was trained on.</p>
<p align="center">&nbsp;</p>
<p>&nbsp;</p>
<p>Examples of bounding box initialisations along with the ground-truth bounding boxes are show in Fig. 5. &nbsp;We provide the <a href="https://ibug.doc.ic.ac.uk/media/uploads/competitions/bounding_boxes.zip">bounding box initialisations</a>, as produced by our in-house detector, for each database of the training procedure. Additionaly the bounding boxes of the ground truth are given.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_5_a.jpg" alt="" width="300" height="225"></td>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_5_b.jpg" alt="" width="300" height="225"></td>
</tr>
</tbody>
</table>
<p style="text-align: center;">Figure 5: Examples of bounding box initialisations for images from the test set of LFPW.</p>
<p>&nbsp;</p>
<p>Participants should expect that initialisations for the 300-W test set are of similar accuracy.</p>
<p>Each binary should accept two inputs: input image (RGB with .png extension) and the coordinates of the bounding box. Bounding box should be a 4x1 vector [xmin, ymin, xmax, ymax] (please see Fig. 6). The output of the binary should be a 68 x 2 matrix with the detected landmarks. This matrix should be saved in the same format (.pts) and ordering as the one of the provided annotations.</p>
<p align="center">&nbsp;</p>
<p style="text-align: center;">&nbsp;&nbsp;<img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/figure_6.png" alt="" width="300" height="307"></p>
<p style="text-align: center;">&nbsp;</p>
<p style="text-align: center;">&nbsp;Figure 6: Coordinates of the bounding box (the coordinates of the top left pixel are x=1, y=1).</p>
<p align="center">&nbsp;</p>
<p>&nbsp;</p>
<p>Facial landmark detection performance will be assessed on both the 68 points mark-up of Fig 1 and the 51 points which correspond to the points without border (please see Fig1). The average point-to-point Euclidean error normalized by the inter-ocular distance (measured as the Euclidean distance between the outer corners of the eyes) will be used as the error measure. Matlab code for calculating the error can be downloaded from <a href="https://ibug.doc.ic.ac.uk/media/uploads/competitions/compute_error.m">http://ibug.doc.ic.ac.uk/media/uploads/competitions/compute_error.m</a>. Finally, the cumulative curve corresponding to the percentage of test images for which the error was less than a specific value will be produced. Additionally, fitting times will be recorded. These results will be returned to the participants for inclusion in their papers.</p>
<p>&nbsp;</p>
<p>The binaries submitted for the competition will be handled confidentially. &nbsp; They will be used only for the scope of the competition and will be erased after the completion. The binaries should be complied in a 64bit machine and dependencies to publicly available vision repositories (such as Open CV) should be explicitly stated in the document that accompanies the binary</p>
<p><strong><br></strong></p>
<p><span style="text-decoration: underline;"><strong>Winners</strong></span></p>
<ul>
<li>J. Yan, Z. Lei, D. Yi, and S. Z. Li.&nbsp;<strong>Learn to combine multiple hypotheses for face alignment</strong>. (Academia)</li>
<li>E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin.&nbsp;<strong>Facial landmark localization with coarse-to-fine convolutional network cascade</strong>.&nbsp;(Industry)</li>
</ul>
<p><span style="text-decoration: underline;"><strong><br></strong></span></p>
<p><span style="text-decoration: underline;"><strong>Results</strong></span></p>
<p><span style="text-decoration: underline;">Indoor</span></p>
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/indoor_51_all.jpg" alt="" width="310" height="223"></td>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/indoor_68_all.jpg" alt="" width="310" height="223"></td>
</tr>
<tr>
<td style="text-align: center;">51 points</td>
<td style="text-align: center;">68 points</td>
</tr>
</tbody>
</table>
<p><span style="text-decoration: underline;"><strong><br></strong></span></p>
<p><span style="text-decoration: underline;">Outdoor</span></p>
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/outdoor_51_all.jpg" alt="" width="310" height="223"></td>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/outdoor_68_all.jpg" alt="" width="310" height="223"></td>
</tr>
<tr>
<td style="text-align: center;">51 points</td>
<td style="text-align: center;">68 points</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;">&nbsp;Indoor + Outdoor</span></p>
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/indoor_outdoor_all_51.jpg" alt="" width="300" height="225"></td>
<td style="text-align: center;"><img src="./i·bug - resources - 300 Faces In-the-Wild Challenge (300-W), ICCV 2013_files/indoor_outdoor_all_68.jpg" alt="" width="300" height="225"></td>
</tr>
<tr>
<td style="text-align: center;">51 points</td>
<td style="text-align: center;">68 points</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Participants</strong></span></p>
<p>1. S. Milborrow, T. Bishop, and F. Nicolls. <strong>Multiview active&nbsp;shape models with sift descriptors for the 300-w face landmark challenge</strong>.</p>
<p>2. S. Jaiswal, T. Almaev, and M. Valstar. <strong>Guided unsupervised learning of mode specific models for facial point detection in the wild</strong>.</p>
<p>3. T. Baltrusaitis, L.-P. Morency, and P. Robinson. <strong>Constrained local neural fields for robust facial landmark detection in the wild</strong>.</p>
<p>4. E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin. <strong>Facial landmark localization with coarse-to-fine convolutional network cascade</strong>.</p>
<p>5. K. Hasan Md., S. Moalem, and C. Pal. <strong>Localizing facial keypoints with global descriptor search, neighbour alignment and locally linear models</strong>.</p>
<p>6. J. Yan, Z. Lei, D. Yi, and S. Z. Li. <strong>Learn to combine multiple hypotheses for face alignment</strong>.</p>
<p><span style="text-decoration: underline;"><strong><br></strong></span></p>
<p><span style="text-decoration: underline;"><strong>Submission Information</strong></span></p>
<p>Challenge participants should submit a paper to the 300-W Workshop, which summarizes the methodology and the achieved performance of their algorithm. Submissions should adhere to the main <a href="http://www.iccv2013.org/author_guidelines.php#formatting">ICCV 2013 proceedings style</a>, and have a maximum length of 8 pages and will be charged a fee if $200, regardless of length. The workshop papers will be published in the ICCV 2013 proceedings. Please sign up in the <a href="https://cmt.research.microsoft.com/W2013/">submissions system</a> to submit your paper.</p>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Important Dates&nbsp;</strong></span></p>
<ul>
<li><span style="text-decoration: line-through;">Binaries submission deadline: September 7, 2013</span></li>
<li><span style="text-decoration: line-through;">Paper submission deadline: September 15, 2013<span style="color: #ff0000;">&nbsp; &nbsp;September 23, 2013 </span> (Extended deadline)<strong><br></strong></span></li>
<li><span style="text-decoration: line-through;">Author Notification: October 7, 2013</span></li>
<li><span style="text-decoration: line-through;">Camera-Ready Papers: <span>October&nbsp;</span>10, 2013</span></li>
</ul>
<p>&nbsp;&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Contact</strong></span></p>
<p>Dr. Georgios Tzimiropoulos<br>gtzimiropoulos@lincoln.ac.uk,&nbsp; gt204@imperial.ac.uk<br>Intelligent Behaviour Understanding Group (iBUG)</p>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>References&nbsp;</strong></span></p>
<p>[1] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.Multi-pie. Image and Vision Computing, 28(5):807–813, 2010.</p>
<p>[2] Belhumeur, P., Jacobs, D., Kriegman, D., Kumar, N.. ‘Localizing parts of faces using a consensus of exemplars’. &nbsp;In Computer Vision and Pattern Recognition, CVPR. (2011).</p>
<p>[3] X. Zhu, D. Ramanan. ‘Face detection, pose estimation and landmark localization in the wild’, Computer Vision and Pattern Recognition (CVPR) Providence, Rhode Island, June 2012.</p>
<p>[4] Vuong Le, Jonathan Brandt, Zhe Lin, Lubomir Boudev, Thomas S. Huang. ‘Interactive Facial Feature Localization’, ECCV2012.</p>
<p>[5] Messer, K., Matas, J., Kittler, J., Luettin, J., Maitre, G. ‘Xm2vtsdb: The ex- tended m2vts database’. In: 2nd international conference on audio and video-based biometric person authentication. Volume 964. (1999).</p>
<p>[6] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou and Maja Pantic. ‘A semi-automatic methodology for facial landmark annotation’, IEEE Int’l Conf. Computer Vision and Pattern Recognition (CVPR-W’13), 5<sup>th</sup>&nbsp;Workshop on Analysis and Modeling of Faces and Gestures (AMFG2013). Portland Oregon, USA, June 2013 (accepted for publication).</p>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Program Committee</strong></span></p>
<ul>
<li><span><span>Fernando De la Torre, Carnegie Mellon University (USA)</span></span></li>
<li><span>Roland Goecke, University of Canberra (AUS)</span></li>
<li>Mircea C. Ionita, Daon (UK)</li>
<li>Qiang Ji, Rensselaer Polytechnic Institute (USA)</li>
<li>Ioannis A. Kakadiaris, University of Houston (USA)</li>
<li>Simon Lucey, CSIRO ICT Centre (AUS)</li>
<li>Brais Martinez, Imperial College London (UK)</li>
<li>Louis-Philippe Morency, USC Los Angeles (USA)</li>
<li>Ioannis (Yiannis) Patras, Queen Mary University (UK)</li>
<li>Jason Saragih, Freelance, (AUS)</li>
<li>Gabor Szirtes, RealEyes (UK / Hungary)</li>
<li>Michel Valstar, University of Nottingham (UK)</li>
<li>Lijun Yin, Binghampton University (USA)</li>
</ul>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;"><strong>Sponsors</strong></span></p>
<h3>EPSRC 4D-FAB Project</h3>
<h3>MAHNOB Project</h3>
        </div>
    </div>
</div>

<div class="clear"></div>

    <div class="clear"> </div>  
    <div class="grid_12">
        <div id="footer">
        <a href="http://www.sanderkoelstra.nl/" class="credits">Design and coding by Sander Koelstra</a>
        <p>Intelligent Behaviour Understanding Group (iBUG), Department of Computing, Imperial College London<br> 
        180 Queen’s Gate, London SW7 2AZ U.K. | Tel: +44-207-594-8195 | Fax: +44-207-581-8024 | </p>
        <div class="clear"> </div>  
        
        </div>
    </div>
</div>




</body></html>